{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f17d3b-cf6f-48ea-9a82-550713b6181a",
   "metadata": {},
   "source": [
    "# 【實務系列】練習用 Tensorflow 打造一個神經網路！\n",
    "~~測試失敗~~\n",
    "\n",
    "參考資料:(https://medium.com/%E7%A8%8B%E5%BC%8F%E6%84%9B%E5%A5%BD%E8%80%85/%E5%AF%A6%E5%8B%99%E7%B3%BB%E5%88%97-%E7%B7%B4%E7%BF%92%E7%94%A8-tensorflow-%E6%89%93%E9%80%A0%E4%B8%80%E5%80%8B%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-9ca637c176a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a9b5e-0551-4190-a9c8-8529a27d79dc",
   "metadata": {},
   "source": [
    "### 第一步：匯入套件跟資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1d05ef-25cf-4708-a5f9-72ab628b051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5908-5d79-496f-a494-f649021b434e",
   "metadata": {},
   "source": [
    "### 第二步：參數設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e41ac10-10f2-40ad-b3f9-e728189d6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 100\n",
    "x_train = x_train / 255.0 # scaler to 0~1\n",
    "x_test = x_test / 255.0 # scaler to 0~1\n",
    "x_test = tf.Variable(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef98efc-98c2-4f4e-b341-9591db1836dc",
   "metadata": {},
   "source": [
    "### 第三步：切分 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3141761b-86e6-4355-9ce3-62ec15556611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def get_batch(x_data, y_data, batch_size):\n",
    "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
    "    return x_data[idxs, :, :], y_data[idxs]\n",
    "\n",
    "x_data, y_data = get_batch(x_train, y_train, batch_size=100)\n",
    "print(y_data.shape)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc4346-564e-4572-9de0-e1493b3cfefb",
   "metadata": {},
   "source": [
    "### 第四步：打造神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889eefa7-8f59-4f60-b09c-4b87ae11b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([784, 300], stddev=0.03, name='w1'))\n",
    "b1 = tf.Variable(tf.random.normal([300]), name='b1')\n",
    "w2 = tf.Variable(tf.random.normal([300, 10], stddev=0.03, name='w2'))\n",
    "b2 = tf.Variable(tf.random.normal([300]), name='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeed71ca-ddff-40f4-8290-8e0c80a1c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(x_input, W1, b1 ,W2, b2):\n",
    "    x_input = tf.reshape(x_input, shape=(x_input.shape[0], -1))\n",
    "    # y = (Wx + b) * activation function\n",
    "    x = tf.add(tf.matmul(tf.cast(x_input, tf.float32), W1), b1)\n",
    "    x = tf.nn.relu(x)\n",
    "    outputs = tf.add(tf.matmul(x, W2), b2)\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c56e45-b182-4a2c-9b5c-b79f3dd37aa7",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3daf83b5-8892-4a7e-bcc4-b2fe8c6d7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_layer = Input(shape=(784,), name='input_layer')\n",
    "# dense = Dense(300, activation='relu', name='hidden_layer')(input_layer)\n",
    "# output_layer = Dense(10, activation='softmax', name='output_layer')(dense)\n",
    "# model_ = Model(inputs=input_layer, outputs=output_layer)\n",
    "# model_.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466cf71-37ca-46b1-9878-86f4e267e80a",
   "metadata": {},
   "source": [
    "### 第五步：定義 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ab51c7-c6f7-482c-adf2-ca1caee92159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=outputs))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1da025-a61a-45b7-aea7-ef667b11d300",
   "metadata": {},
   "source": [
    "### 第六步：定義 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37dd2f91-74c4-4e9c-bc04-b2941a31dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133db84-4ebf-48f6-95d7-0d1fe623b93c",
   "metadata": {},
   "source": [
    "### 第七步：訓練網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4439c80f-f324-47d1-a2ea-1678f6336d7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100,10] vs. [300] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(batch_y, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 11\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m=\u001b[39mpred, labels\u001b[38;5;241m=\u001b[39mbatch_y)\n\u001b[0;32m     14\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, [w1, b1, w2, b2])\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mnn_model\u001b[1;34m(x_input, W1, b1, W2, b2)\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mmatmul(tf\u001b[38;5;241m.\u001b[39mcast(x_input, tf\u001b[38;5;241m.\u001b[39mfloat32), W1), b1)\n\u001b[0;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(outputs)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_py39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_py39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100,10] vs. [300] [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "total_batch = int(len(y_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = get_batch(x_train, y_train, batch_size=batch_size)\n",
    "        batch_x = tf.Variable(batch_x)\n",
    "        # batch_y.shape = (100, )\n",
    "        batch_y = tf.Variable(batch_y)\n",
    "        batch_y = tf.one_hot(batch_y, 10)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = nn_model(batch_x, w1, b1, w2, b2)\n",
    "            loss = loss_fn(outputs=pred, labels=batch_y)\n",
    "        \n",
    "        gradients = tape.gradient(loss, [w1, b1, w2, b2])\n",
    "        optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2]))\n",
    "        avg_loss += loss / total_batch\n",
    "    \n",
    "    # validate training data\n",
    "    train_pred = nn_model(x_train, w1, b1, w2, b2)\n",
    "    train_max_idxs = tf.argmax(train_pred, axis=1)\n",
    "    acc = np.sum(train_max_idxs.numpy() == y_train) / len(y_train)\n",
    "    \n",
    "    # validate testing data\n",
    "    test_pred = nn_model(x_test, w1, b1, w2, b2)\n",
    "    y_test_one_hot = tf.one_hot(y_test, 10)\n",
    "    test_loss = loss_fn(outputs=test_pred, labels=y_test_one_hot)\n",
    "    max_idxs = tf.argmax(test_pred, axis=1)\n",
    "    test_acc = np.sum(max_idxs.numpy() == y_test) / len(y_test)\n",
    "    print(f\"Epoch: {epoch+1}, loss={avg_loss:.3f}, , acc={acc:.3f}, val_loss={test_loss:.3f}, val_acc:{test_acc*100:.3f}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
